{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19: Face Mask Detector with OpenCV, Keras/TensorFlow, and Deep Learning\n",
    "\n",
    "\n",
    "### Two-phase COVID-19 face mask detector\n",
    "\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_phases.png)\n",
    "\n",
    "* In order to train a custom face mask detector, we need to break our project into two distinct phases, each with its own respective sub-steps (as shown in Figure above):\n",
    "\n",
    "1. Training: Here we’ll focus on loading our face mask detection dataset from disk, training a model (using Keras/TensorFlow) on this dataset, and then serializing the face mask detector to disk.\n",
    "\n",
    "2. Deployment: Once the face mask detector is trained, we can then move on to loading the mask detector, performing face detection, and then classifying each face as with_mask or without_mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our COVID-19 face mask detection dataset\n",
    "\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_dataset.jpg)\n",
    "\n",
    "#### This dataset consists of 1,376 images belonging to two classes:\n",
    "\n",
    "* with_mask: 690 images\n",
    "* without_mask: 686 images\n",
    "\n",
    "<b>Our goal is to train a custom deep learning model to detect whether a person is or is not wearing a mask.</b>\n",
    "\n",
    "* Facial landmarks allow us to automatically infer the location of facial structures, including:\n",
    "\n",
    " * Eyes\n",
    " * Eyebrows\n",
    " * Nose\n",
    " * Mouth\n",
    " * Jawline\n",
    "\n",
    "<b>To use facial landmarks to build a dataset of faces wearing face masks, we need to first start with an image of a person not wearing a face mask:</b>\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_input.jpg)\n",
    "\n",
    "<b>From there, we apply face detection to compute the bounding box location of the face in the image:</b>\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_face_detected.jpg)\n",
    "\n",
    "<b>Once we know where in the image the face is, we can extract the face Region of Interest (ROI):<b>\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_face_extracted.jpg)\n",
    "    \n",
    "<b>And from there, we apply facial landmarks, allowing us to localize the eyes, nose, mouth, etc.:</b>\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_facial_landmarks.png)\n",
    "\n",
    "<b>Next, we need an image of a mask (with a transparent background) such as the one below:</b>\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_face_augment_mask.png)\n",
    "\n",
    "<b>This mask will be automatically applied to the face by using the facial landmarks (namely the points along the chin and nose) to compute where the mask will be placed.\n",
    "\n",
    "The mask is then resized and rotated, placing it on the face:</b>\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_augmented_with_mask.jpg)\n",
    "\n",
    "<b>We can then repeat this process for all of our input images, thereby creating our artificial face mask dataset:</b>\n",
    "![](../image/Face-Mask-Detection/face_mask_detection_masks_montage.jpg)\n",
    "\n",
    "* However, there is a caveat you should be aware of when using this method to artificially create a dataset!\n",
    "\n",
    " 1. If you use a set of images to create an artificial dataset of people wearing masks, you cannot “re-use” the images without  masks in your training set — you still need to gather non-face mask images that were not used in the artificial generation process!\n",
    "\n",
    " 2. If you include the original images used to generate face mask samples as non-face mask samples, your model will become heavily biased and fail to generalize well. Avoid that at all costs by taking the time to gather new examples of faces without masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structure\n",
    "\n",
    "├── dataset\n",
    "│   ├── with_mask [690 entries]\n",
    "│   └── without_mask [686 entries]\n",
    "├── examples\n",
    "│   ├── example_01.png\n",
    "│   ├── example_02.png\n",
    "│   └── example_03.png\n",
    "├── face_detector\n",
    "│   ├── deploy.prototxt\n",
    "│   └── res10_300x300_ssd_iter_140000.caffemodel\n",
    "├── detect_mask_image.py\n",
    "├── detect_mask_video.py\n",
    "├── mask_detector.model\n",
    "├── plot.png\n",
    "└── train_mask_detector.py\n",
    "\n",
    "* Three image examples/ are provided so that you can test the static image face mask detector.\n",
    "\n",
    "* We’ll be reviewing three Python scripts in this tutorial:\n",
    "\n",
    " 1. train_mask_detector.py: Accepts our input dataset and fine-tunes MobileNetV2 upon it to create our mask_detector.model. A training history plot.png containing accuracy/loss curves is also produced\n",
    " 2. detect_mask_image.py: Performs face mask detection in static images\n",
    " 3. detect_mask_video.py: Using your webcam, this script applies face mask detection to every frame in the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing our COVID-19 face mask detector training script with Keras and TensorFlow\n",
    "\n",
    "* Now that we’ve reviewed our face mask dataset, let’s learn how we can use Keras and TensorFlow to train a classifier to automatically detect whether a person is wearing a mask or not.\n",
    "\n",
    "* To accomplish this task, we’ll be fine-tuning the MobileNet V2 architecture, a highly efficient architecture that can be applied to embedded devices with limited computational capacity (ex., Raspberry Pi, Google Coral, NVIDIA Jetson Nano, etc.).\n",
    "\n",
    "* Deploying our face mask detector to embedded devices could reduce the cost of manufacturing such face mask detection systems, hence why we choose to use this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_mask_detector.py\n",
    "\n",
    "* Our set of tensorflow.keras imports allow for:\n",
    "\n",
    " 1. Data augmentation\n",
    " 2. Loading the MobilNetV2 classifier (we will fine-tune this model with pre-trained ImageNet weights)\n",
    " 3. Building a new fully-connected (FC) head\n",
    " 5. Pre-processing\n",
    " 6. Loading image data\n",
    "\n",
    "* We’ll use scikit-learn (sklearn) for binarizing class labels, segmenting our dataset, and printing a classification report.\n",
    "\n",
    "* The imutils paths implementation will help us to find and list images in our dataset. And we’ll use matplotlib to plot our training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The command line arguments include:\n",
    "\n",
    " 1. --dataset: The path to the input dataset of faces and and faces with masks\n",
    " 2. --plot: The path to your output training history plot, which will be generated using matplotlib\n",
    " 3. --model: The path to the resulting serialized face mask classification model\n",
    "\n",
    "* Define the deep learning hyperparameters in one place:\n",
    "  Specify the hyperparameter constants including the initial learning rate(INIT_LR), number of training epochs, and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
    "#     help=\"path to input dataset\")\n",
    "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "#     help=\"path to output loss/accuracy plot\")\n",
    "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
    "#     default=\"mask_detector.model\",\n",
    "#     help=\"path to output face mask detector model\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "dataset = \"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face-mask-dataset/dataset/train\"\n",
    "plot = \"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/plot.png\"\n",
    "model = \"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/mask_detector.model\"\n",
    "    \n",
    "# initialize the initial learning rate, number of epochs to train for,\n",
    "# and batch size\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load and pre-process the training data:\n",
    "\n",
    " 1. Grab all of the imagePaths in the dataset.\n",
    " 2. Initialize data and labels lists.\n",
    " 3. Loop over the imagePaths and loading + pre-processing images. Pre-processing steps include resizing to 224×224 pixels, conversion to array format, and scaling the pixel intensities in the input image to the range [-1, 1] (via the preprocess_input convenience function)\n",
    " 4. Append the pre-processed image and associated label to the data and labels lists, respectively.\n",
    " 5. Ensure the training data is in NumPy array format.\n",
    " 6. We assume that the entire dataset is small enough to fit into memory. If the dataset is larger than the memory available, we suggest using HDF5.\n",
    "\n",
    "* Next, we’ll encode our labels, partition our dataset, and prepare for data augmentation:\n",
    "\n",
    "* One-hot encode our class labels, meaning that our data will be in the following format:\n",
    "\n",
    "COVID-19: Face Mask Detector with OpenCV, Keras/TensorFlow, and Deep Learning\n",
    "$ python  train_mask_detector.py --dataset  dataset \n",
    "[INFO] loading images...\n",
    "-> (trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "(Pdb) labels[500:]\n",
    "array([[1., 0.],\n",
    "       [1., 0.],\n",
    "       [1., 0.],\n",
    "       ...,\n",
    "       [0., 1.],\n",
    "       [0., 1.],\n",
    "       [0., 1.]], dtype=float32)\n",
    "(Pdb)\n",
    "\n",
    "* As you can see, each element of our labels array consists of an array in which only one index is “hot” (i.e., 1).\n",
    "\n",
    "* Using scikit-learn’s convenience method, segment our data into 80% training and the remaining 20% for testing.\n",
    "\n",
    "* During training, we’ll be applying on-the-fly mutations to our images in an effort to improve generalization. This is known as  data augmentation, where the random rotation, zoom, shear, shift, and flip parameters are established. We’ll use the aug object at training time.\n",
    "\n",
    "* But first, we need to prepare MobileNetV2 for fine-tuning:\n",
    "\n",
    "* Fine-tuning setup is a three-step process:\n",
    "\n",
    " 1. Load MobileNet with pre-trained ImageNet weights, leaving off head of network.\n",
    " 2. Construct a new FC head, and append it to the base in place of the old head.\n",
    " 2. Freeze the base layers of the network. The weights of these base layers will not be updated during the process of backpropagation, whereas the head layer weights will be tuned.\n",
    "\n",
    "* Fine-tuning is a strategy, which is nearly always recommend to establish a baseline model while saving considerable time.\n",
    "\n",
    "* With the data prepared and model architecture in place for fine-tuning, we’re now ready to compile and train our face mask detector network:\n",
    "\n",
    " 1. Compile the model with the Adam optimizer, a learning rate decay schedule, and binary cross-entropy. If you’re building from this training script with > 2 classes, be sure to use categorical cross-entropy.\n",
    "\n",
    " 2. Face mask training is launched. Notice how our data augmentation object (aug) will be providing batches of mutated image data.\n",
    "\n",
    "* Once training is complete, we’ll evaluate the resulting model on the test set:\n",
    " \n",
    " 1. Here, make predictions on the test set, grabbing the highest probability class label indices. Then, we print a classification report in the terminal for inspection.\n",
    "\n",
    "2. Serializes our face mask classification model to disk.\n",
    "\n",
    "* The last step is to plot the accuracy and loss curves:\n",
    "* Once the plot is ready, save the figure to disk using the --plot filepath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the list of images in our dataset directory, then initialize\n",
    "# the list of data (i.e., images) and class images\n",
    "print(\"[INFO] loading images...\")\n",
    "#imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "imagePaths = list(paths.list_images(dataset))\n",
    "#imagePaths = list(paths.list_images(\"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face-mask-dataset/dataset/train\"))\n",
    "data = []\n",
    "labels = []\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    print(\"imagePath =  \", imagePath)\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    print(\"label =  \", label)\n",
    "    \n",
    "    # load the input image (224x224) and preprocess it\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size=0.20, stratify=labels, random_state=42)\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
    "# left off\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# compile our model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS)\n",
    "\n",
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "    target_names=lb.classes_))\n",
    "\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] saving mask detector model...\")\n",
    "# model.save(args[\"model\"], save_format=\"h5\")\n",
    "model.save(model, save_format=\"h5\")\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.savefig(args[\"plot\"])\n",
    "plt.savefig(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the COVID-19 face mask detector with Keras/TensorFlow\n",
    "\n",
    "* We are now ready to train our face mask detector using Keras, TensorFlow, and Deep Learning.\n",
    "* Open up a terminal, and execute the following command:<br>\n",
    " $ python train_mask_detector.py --dataset dataset\n",
    " \n",
    "* As you can see, we are obtaining ~99% accuracy on our test set.\n",
    "\n",
    "* Looking at COVID-19 face mask detector training accuracy/loss curves, we can see that there are little signs of overfitting, with the validation loss lower than the training loss.\n",
    "\n",
    "* Given these results, we are hopeful that our model will generalize well to images outside our training and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing our COVID-19 face mask detector for images with OpenCV\n",
    "\n",
    "* Now that our face mask detector is trained, let’s learn how we can:\n",
    "\n",
    " 1. Load an input image from disk\n",
    " 2. Detect faces in the image\n",
    " 3. Apply our face mask detector to classify the face as either with_mask or without_mask\n",
    " \n",
    "* detect_mask_image.py\n",
    "\n",
    " * Our driver script requires three TensorFlow/Keras imports to (1) load our MaskNet model and (2) pre-process the input image.\n",
    "\n",
    " * OpenCV is required for display and image manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next step is to parse command line arguments:\n",
    "* Our four command line arguments include:\n",
    "\n",
    " * --image: The path to the input image containing faces for inference\n",
    " * --face: The path to the face detector model directory (we need to localize faces prior to classifying them)\n",
    " * --model: The path to the face mask detector model that we trained earlier in this tutorial\n",
    " * --confidence: An optional probability threshold can be set to override 50% to filter weak face detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "#     help=\"path to input image\")\n",
    "# ap.add_argument(\"-f\", \"--face\", type=str,\n",
    "#     default=\"face_detector\",\n",
    "#     help=\"path to face detector model directory\")\n",
    "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
    "#     default=\"mask_detector.model\",\n",
    "#     help=\"path to trained face mask detector model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "#     help=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we’ll load both our face detector and face mask classifier models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector model...\n",
      "[INFO] loading face mask detector model...\n"
     ]
    }
   ],
   "source": [
    "default_confidence = 0.5\n",
    "# load our serialized face detector model from disk\n",
    "print(\"[INFO] loading face detector model...\")\n",
    "#prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
    "prototxtPath = \"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/deploy.prototxt\"\n",
    "#weightsPath = os.path.sep.join([args[\"face\"], \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "weightsPath = \"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "net = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# load the face mask detector model from disk\n",
    "print(\"[INFO] loading face mask detector model...\")\n",
    "# model = load_model(args[\"model\"])\n",
    "model = load_model(\"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/mask_detector.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With our deep learning models now in memory, our next step is to load and pre-process an input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] computing face detections...\n"
     ]
    }
   ],
   "source": [
    "# load the input image from disk, clone it, and grab the image spatial dimensions\n",
    "#image = cv2.imread(args[\"image\"])\n",
    "#image = cv2.imread(\"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face-mask-dataset/dataset/test/with_mask/480-with-mask.jpg\")\n",
    "image = cv2.imread(\"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face-mask-dataset/dataset/test/without_mask/409.jpg\")\n",
    "orig = image.copy()\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# construct a blob from the image\n",
    "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "# pass the blob through the network and obtain the face detections\n",
    "print(\"[INFO] computing face detections...\")\n",
    "net.setInput(blob)\n",
    "detections = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upon loading our --image from disk, we make a copy and grab frame dimensions for future scaling and display purposes.\n",
    "\n",
    "* Pre-processing is handled by OpenCV’s blobFromImage function. As shown in the parameters, we resize to 300×300 pixels and  perform mean subtraction.\n",
    "\n",
    "* Perform face detection to localize where in the image all faces are.\n",
    "\n",
    "* Once we know where each face is predicted to be, we’ll ensure they meet the --confidence threshold before we extract the faceROIs:\n",
    "\n",
    " * Here, we loop over our detections and extract the confidence to measure against the --confidence threshold.\n",
    "\n",
    " * We then compute bounding box value for a particular face and ensure that the box falls within the boundaries of the image.\n",
    " \n",
    "* Next, we’ll run the face ROI through our MaskNet model:\n",
    "\n",
    "* In this block, we:\n",
    "\n",
    " 1. Extract the face ROI via NumPy slicing.\n",
    " 2. Pre-process the ROI the same way we did during training.\n",
    " 3. Perform mask detection to predict with_mask or without_mask.\n",
    " 4. From here, we will annotate and display the result!\n",
    " \n",
    "* First, we determine the class label based on probabilities returned by the mask detector model and assign an associated color for the annotation. The color will be “green” for with_mask and “red” for without_mask.\n",
    "\n",
    "* We then draw the label text (including class and probability), as well as a bounding box rectangle for the face, using OpenCV drawing functions.\n",
    "\n",
    "* Once all detections have been processed, display the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the detections\n",
    "for i in range(0, detections.shape[2]):\n",
    "    # extract the confidence (i.e., probability) associated with the detection\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    # filter out weak detections by ensuring the confidence is greater than the minimum confidence\n",
    "    #if confidence > args[\"confidence\"]:\n",
    "    if confidence > default_confidence:\n",
    "        # compute the (x, y)-coordinates of the bounding box for the object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        # ensure the bounding boxes fall within the dimensions of the frame\n",
    "        (startX, startY) = (max(0, startX), max(0, startY))\n",
    "        (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "        \n",
    "        # extract the face ROI, convert it from BGR to RGB channel ordering, resize it to 224x224, and preprocess it\n",
    "        face = image[startY:endY, startX:endX]\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        face = cv2.resize(face, (224, 224))\n",
    "        face = img_to_array(face)\n",
    "        face = preprocess_input(face)\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "        # pass the face through the model to determine if the face has a mask or not\n",
    "        (mask, withoutMask) = model.predict(face)[0]\n",
    "        \n",
    "        # determine the class label and color we'll use to draw the bounding box and text\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        # include the probability in the label\n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        \n",
    "        # display the label and bounding box rectangle on the output frame\n",
    "        cv2.putText(image, label, (startX, startY - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
    "\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19 face mask detection in images with OpenCV\n",
    "\n",
    "* Let’s put our COVID-19 face mask detector to work!\n",
    "\n",
    "* Open up a terminal, and execute the following command:<br>\n",
    "\n",
    "* $ python detect_mask_image.py --image examples/example_01.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing our COVID-19 face mask detector in real-time video streams with OpenCV\n",
    "\n",
    "* detect_mask_video.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The algorithm for this script is the same, but it is pieced together in such a way to allow for processing every frame of your webcam stream.\n",
    "\n",
    "* Thus, the only difference when it comes to imports is that we need a VideoStream class and time. Both of these will help us to work with the stream. We’ll also take advantage of imutils for its aspect-aware resizing method.\n",
    "\n",
    "* Our face detection/mask prediction logic for this script is in the <b>detect_and_predict_mask</b> function:\n",
    "\n",
    "* By defining this convenience function here, our frame processing loop will be a little easier to read later.\n",
    "\n",
    "* This function detects faces and then applies our face mask classifier to each face ROI. Such a function consolidates our code — it could even be moved to a separate Python file if you so choose.\n",
    "\n",
    "* Our <b>detect_and_predict_mask</b> function accepts three parameters:\n",
    "\n",
    " * frame: A frame from our stream\n",
    " * faceNet: The model used to detect where in the image faces are\n",
    " * maskNet: Our COVID-19 face mask classifier model\n",
    "\n",
    "* Inside, we construct a blob, detect faces, and initialize lists, two of which the function is set to return. These lists include our faces (i.e., ROIs), locs (the face locations), and preds (the list of mask/no mask predictions).\n",
    "\n",
    "* From here, we’ll loop over the face detections:\n",
    " * Inside the loop, we filter out weak detections and extract bounding boxes while ensuring bounding box coordinates do not fall outside the bounds of the image.\n",
    "\n",
    "* Next, we’ll add face ROIs to two of our corresponding lists:\n",
    "\n",
    "* After extracting face ROIs and pre-processing, we append the the face ROIs and bounding boxes to their respective lists.\n",
    "\n",
    "* We’re now ready to run our faces through our mask predictor:\n",
    "\n",
    "* The logic here is built for speed. First we ensure at least one face was detected — if not, we’ll return empty preds.\n",
    "\n",
    "* Secondly, we are performing inference on our entire batch of faces in the frame so that our pipeline is faster. It wouldn’t make sense to write another loop to make predictions on each face individually due to the overhead (especially if you are using a GPU that requires a lot of overhead communication on your system bus). It is more efficient to perform predictions in batch.\n",
    "\n",
    "* Finally, it returns our face bounding box locations and corresponding mask/not mask predictions to the caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "    # grab the dimensions of the frame and then construct a blob from it\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0))\n",
    "    \n",
    "    # pass the blob through the network and obtain the face detections\n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "    # initialize our list of faces, their corresponding locations, and the list of predictions from our face mask network\n",
    "    faces = []\n",
    "    locs = []\n",
    "    preds = []\n",
    "    \n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the detection\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the confidence is greater than the minimum confidence\n",
    "        #if confidence > args[\"confidence\"]:\n",
    "        if confidence > default_confidence:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            \n",
    "            # ensure the bounding boxes fall within the dimensions of the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "            \n",
    "            # extract the face ROI, convert it from BGR to RGB channel\n",
    "            # ordering, resize it to 224x224, and preprocess it\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            face = cv2.resize(face, (224, 224))\n",
    "            face = img_to_array(face)\n",
    "            face = preprocess_input(face)\n",
    "            \n",
    "            # add the face and bounding boxes to their respective lists\n",
    "            faces.append(face)\n",
    "            locs.append((startX, startY, endX, endY))\n",
    "    \n",
    "    # only make a predictions if at least one face was detected\n",
    "    if len(faces) > 0:\n",
    "        # for faster inference we'll make batch predictions on *all*\n",
    "        # faces at the same time rather than one-by-one predictions in the above `for` loop\n",
    "        faces = np.array(faces, dtype=\"float32\")\n",
    "        preds = maskNet.predict(faces, batch_size=32)\n",
    "        \n",
    "    # return a 2-tuple of the face locations and their corresponding locations\n",
    "    return (locs, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we’ll define our command line arguments:\n",
    "\n",
    "* Our command line arguments include:\n",
    "\n",
    " * --face: The path to the face detector directory\n",
    " * --model: The path to our trained face mask classifier\n",
    " * --confidence: The minimum probability threshold to filter weak face detections\n",
    "\n",
    "* With our imports, convenience function, and command line args ready to go, we just have a few initializations to handle before we loop over frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-f\", \"--face\", type=str,\n",
    "#     default=\"face_detector\",\n",
    "#     help=\"path to face detector model directory\")\n",
    "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
    "#     default=\"mask_detector.model\",\n",
    "#     help=\"path to trained face mask detector model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "#     help=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector model...\n",
      "[INFO] loading face mask detector model...\n",
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "default_confidence = 0.5\n",
    "# load our serialized face detector model from disk\n",
    "print(\"[INFO] loading face detector model...\")\n",
    "# prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
    "prototxtPath = \"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/deploy.prototxt\"\n",
    "# weightsPath = os.path.sep.join([args[\"face\"], \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "weightsPath = \"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# load the face mask detector model from disk\n",
    "print(\"[INFO] loading face mask detector model...\")\n",
    "# maskNet = load_model(args[\"model\"])\n",
    "maskNet = load_model(\"D:/Python/pyimagesearch-Opencv-tutorials/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/face_detector/mask_detector.model\")\n",
    "\n",
    "# initialize the video stream and allow the camera sensor to warm up\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we have initialized our:\n",
    "\n",
    " * Face detector\n",
    " * COVID-19 face mask detector\n",
    " * Webcam video stream\n",
    "\n",
    "* Let’s proceed to loop over frames in the stream:\n",
    "\n",
    "* We begin looping over frames. Inside, we grab a frame from the stream and resize it.\n",
    "\n",
    "* From there, we put our convenience utility to use; Then detect and predict whether people are wearing their masks or not.\n",
    "\n",
    "* Let’s post-process (i.e., annotate) the COVID-19 face mask detection results:\n",
    "\n",
    "* Inside our loop over the prediction results, we:\n",
    "\n",
    " * Unpack a face bounding box and mask/not mask prediction\n",
    " * Determine the label and color\n",
    " * Annotate the label and face bounding box\n",
    "\n",
    "* Finally, we display the results and perform cleanup:\n",
    "\n",
    "* After the frame is displayed, we capture key presses. If the user presses q (quit), we break out of the loop and perform housekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream and resize it\n",
    "    # to have a maximum width of 400 pixels\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "    # detect faces in the frame and determine if they are wearing a\n",
    "    # face mask or not\n",
    "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "    \n",
    "    # loop over the detected face locations and their corresponding locations\n",
    "    for (box, pred) in zip(locs, preds):\n",
    "        # unpack the bounding box and predictions\n",
    "        (startX, startY, endX, endY) = box\n",
    "        (mask, withoutMask) = pred\n",
    "        # determine the class label and color we'll use to draw the bounding box and text\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        # include the probability in the label\n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        # display the label and bounding box rectangle on the output frame\n",
    "        cv2.putText(frame, label, (startX, startY - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        \n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stream.release()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting COVID-19 face masks with OpenCV in real-time\n",
    "\n",
    "* We can launch the mask detector in real-time video streams using the following command:<br>\n",
    "$ python detect_mask_video.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
