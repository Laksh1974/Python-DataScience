{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Drowsiness detection with OpenCV\n",
    "\n",
    "\n",
    "### The drowsiness detector algorithm\n",
    "\n",
    "##### The general flow of drowsiness detection algorithm is fairly straightforward.\n",
    "\n",
    "##### First, we’ll setup a camera that monitors a stream for faces:\n",
    "1. Look for faces in the input video stream.\n",
    "2. Apply facial landmark detection to extract the eye regions from the face.\n",
    "3. Compute the eye aspect ratio to determine if the eyes are closed.\n",
    "4. Sound an alarm if the eyes have been closed for a sufficiently long enough time.\n",
    "\n",
    "Required Packages: \n",
    "1. <b>'OpenCV'</b>\n",
    "2. The <b>'SciPy'</b> package is used to compute the Euclidean distance between facial landmarks points in the eye aspect ratio calculation.<\n",
    "3. The <b>'Thread'</b> class is used to play alarm in a separate thread from the main thread to ensure our script doesn’t pause execution while the alarm sounds.\n",
    "4. The <b>'playsound'</b> library is used in order to play WAV/MP3 alarm sound, which is a pure Python, cross-platform implementation for playing simple sounds.\n",
    "5. The <b>'dlib'</b> library is used to detect and localize facial landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import playsound\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The <b>sound_alarm()</b> function accepts a path to an audio file residing on disk and then plays the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sound_alarm(path):\n",
    "    # play an alarm sound\n",
    "    playsound.playsound(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The <b>eye_aspect_ratio()</b> function compute the ratio of distances between the vertical eye landmarks and the distances between the horizontal eye landmarks.\n",
    "* The return value of the eye aspect ratio will be approximately constant when the eye is open. \n",
    "* The value will then rapid decrease towards zero during a blink.\n",
    "* If the eye is closed, the eye aspect ratio will again remain approximately constant, but will be much smaller than the ratio when the eye is open.\n",
    "\n",
    "![](../image/Drowsiness-Detection/blink_detection_plot.jpg)\n",
    "\n",
    "* On the top-left we have an eye that is fully open with the eye facial landmarks plotted. \n",
    "* On the top-right we have an eye that is closed. \n",
    "* The bottom then plots the eye aspect ratio over time.\n",
    "\n",
    "* As we can see, the eye aspect ratio is constant (indicating the eye is open), then rapidly drops to zero, then increases again, indicating a blink has taken place.\n",
    "\n",
    "* In our drowsiness detector case, we’ll be monitoring the eye aspect ratio to see if the value falls but does not increase again, thus implying that the person has closed their eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    # compute the euclidean distance between the horizontal\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    # compute the eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    # return the eye aspect ratio\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_predictor : This is the path to dlib’s pre-trained facial landmark detector. \n",
    "shape_predictor = \"../models/Drowsiness Detection/shape_predictor_68_face_landmarks.dat\"\n",
    "#alarm : The path to an input audio file to be used as an alarm\n",
    "alarm = \"../alarm/alarm.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the eye aspect ratio falls below 'EYE_AR_THRESH' value, we’ll start counting the number of frames the person has closed their eyes for. (An EYE_AR_THRESH  of 0.3  works well in a variety of situations)\n",
    "* If the number of frames the person has closed their eyes in exceeds EYE_AR_CONSEC_FRAMES, we’ll sound an alarm.\n",
    "* We have set the EYE_AR_CONSEC_FRAMES  to be 48, meaning that if a person has closed their eyes for 48 consecutive frames, we’ll play the alarm sound.\n",
    "* You can make the drowsiness detector more sensitive by decreasing the EYE_AR_CONSEC_FRAMES — similarly, you can make the drowsiness detector less sensitive by increasing it.\n",
    "* COUNTER, the total number of consecutive frames where the eye aspect ratio is below EYE_AR_THRESH.\n",
    "* If COUNTER exceeds EYE_AR_CONSEC_FRAMES, then we’ll update the boolean ALARM_ON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two constants, one for the eye aspect ratio to indicate\n",
    "# blink and then a second constant for the number of consecutive\n",
    "# frames the eye must be below the threshold for to set off the\n",
    "# alarm\n",
    "EYE_AR_THRESH = 0.3\n",
    "#EYE_AR_CONSEC_FRAMES = 48\n",
    "EYE_AR_CONSEC_FRAMES = 20\n",
    "# initialize the frame counter as well as a boolean used to\n",
    "# indicate if the alarm is going off\n",
    "COUNTER = 0\n",
    "ALARM_ON = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dlib library ships with a Histogram of Oriented Gradients-based face detector along with a facial landmark predictor<br> we instantiate both of these in the following block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading facial landmark predictor...\n"
     ]
    }
   ],
   "source": [
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The facial landmarks produced by dlib are an indexable list, as shown below:\n",
    "![](../image/Drowsiness-Detection/facial_landmarks_68markup-768x619.jpg)\n",
    "\n",
    "* Therefore, to extract the eye regions from a set of facial landmarks, we simply need to know the correct array slice indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the indexes of the facial landmarks for the left and\n",
    "# right eye, respectively\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using these indexes, we’ll easily be able to extract the eye regions via an array slice.\n",
    "* We are now ready to start the core of our drowsiness detector:\n",
    "\n",
    " 1. Instantiate our VideoStream.\n",
    " 2. Pause for a second to allow the camera sensor to warm up.\n",
    " 3. Start looping over frames in the video stream.\n",
    " 4. Read the next frame, which we then preprocess by resizing it to have a width of 450 pixels and converting it to grayscale.\n",
    " 5. Apply dlib’s face detector to find and locate the face(s) in the image.<br><br>\n",
    " \n",
    " * The next step is to apply facial landmark detection to localize each of the important regions of the face:\n",
    " \n",
    " 1. Loop over each of the detected faces. We assume that there is only one face — the driver. But we left this for  loop in here just in case we want to apply the technique to videos with more than one face.\n",
    "\n",
    " 2. For each of the detected faces, we apply dlib’s facial landmark detector and convert the result to a NumPy array.\n",
    "\n",
    " 3. Using NumPy array slicing we can extract the (x, y)-coordinates of the left and right eye, respectively.\n",
    "\n",
    " 4. Given the (x, y)-coordinates for both eyes, we then compute their eye aspect ratios.\n",
    "\n",
    " 5. Soukupová and Čech recommend averaging both eye aspect ratios together to obtain a better estimation.\n",
    "\n",
    " 6. We can then visualize each of the eye regions on our frame  by using the cv2.drawContours function below — this is often   helpful when we are trying to debug our script and want to ensure that the eyes are being correctly detected and localized:\n",
    " \n",
    "* Finally, we are ready to check to see if the person in our video stream is starting to show symptoms of drowsiness:\n",
    " \n",
    " 1. Make a check to see if the eye aspect ratio is below the “blink/closed” eye threshold, EYE_AR_THRESH.\n",
    "\n",
    " 2. If it is, we increment COUNTER, the total number of consecutive frames where the person has had their eyes closed.\n",
    "\n",
    " 3. If COUNTER exceeds EYE_AR_CONSEC_FRAMES, then we assume the person is starting to doze off.\n",
    "\n",
    " 4. Another check is made, this time to see if the alarm is on — if it’s not, we turn it on.\n",
    "\n",
    " 5. Handle playing the alarm sound, provided an --alarm  path was supplied when the script was executed. \n",
    "    We take special care to create a separate thread responsible for calling sound_alarm  to ensure that our main program isn’t     blocked until the sound finishes playing.\n",
    "\n",
    " 6. Draw the text DROWSINESS ALERT!  on our frame  — again, this is often helpful for debugging, especially if you are not using     the playsound  library.\n",
    "\n",
    " 7. Finally, handle the case where the eye aspect ratio is larger than EYE_AR_THRESH, indicating the eyes are open. \n",
    "    If the eyes are open, we reset COUNTER  and ensure the alarm is off.\n",
    "\n",
    "* The final code block in our drowsiness detector handles displaying the output frame  to our screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream thread...\n"
     ]
    }
   ],
   "source": [
    "# start the video stream thread\n",
    "print(\"[INFO] starting video stream thread...\")\n",
    "vs = VideoStream(0).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "# loop over frames from the video stream\n",
    "while True:\n",
    "# grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale\n",
    "    # channels)\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # detect faces in the grayscale frame\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # loop over the face detections\n",
    "    for rect in rects:\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        # extract the left and right eye coordinates, then use the\n",
    "        # coordinates to compute the eye aspect ratio for both eyes\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "        # average the eye aspect ratio together for both eyes\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "        \n",
    "        # compute the convex hull for the left and right eye, then\n",
    "        # visualize each of the eyes\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "        \n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    "            # if the eyes were closed for a sufficient number of\n",
    "            # then sound the alarm\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                # if the alarm is not on, turn it on\n",
    "                if not ALARM_ON:\n",
    "                    ALARM_ON = True\n",
    "                    # check to see if an alarm file was supplied,\n",
    "                    # and if so, start a thread to have the alarm\n",
    "                    # sound played in the background\n",
    "                    if alarm != \"\":\n",
    "                        t = Thread(target=sound_alarm,\n",
    "                            args=(alarm,))                                   \n",
    "                        t.deamon = True\n",
    "                        t.start()\n",
    "                # draw an alarm on the frame\n",
    "                cv2.putText(frame, \"DROWSINESS ALERT!\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        # otherwise, the eye aspect ratio is not below the blink\n",
    "        # threshold, so reset the counter and alarm\n",
    "        else:\n",
    "            COUNTER = 0\n",
    "            ALARM_ON = False\n",
    "            \n",
    "        # draw the computed eye aspect ratio on the frame to help\n",
    "        # with debugging and setting the correct eye aspect ratio\n",
    "        # thresholds and frame counters\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    " \n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "vs.stream.release()\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
